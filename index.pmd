% CSC 722 HW 6
% Jon Craton

Problem 1
=========

> In the following table, xi represents the score for the math class and yi represents the score for English class.

```python
x = [95,85,80,70,60]
y = [85,95,70,65,70]
```

Part A
------

> Find the linear regression for the x and y. (y = mx + b). Please write your whole solution.

The idea of linear regression is to find a line of the following form:

$y = mx + b$

`m` and `b` should be found to minimize total distance between the dataset and the line.

The square of the distance between each datapoint and the line can be represented as:

$D = (yᵢ - mxᵢ - b)²$

The total distance would therefore be:

$D = \sum_{i=1}^{n}(yᵢ - mxᵢ - b)²$

In order to minimize the above equation, we will begin by taking the partial derivatives with respect to `m` and `b`. Let's start with `b`.

${∂D \over ∂b} = {∂ \over ∂b}\sum_{i=1}^{n}(yᵢ - mxᵢ - b)²$

${∂D \over ∂b} = -2(\sum_{i=1}^{n}yᵢ - \sum_{i=1}^{n}mxᵢ - \sum_{i=1}^{n}b)$

${∂D \over ∂b} = -2(\sum_{i=1}^{n}yᵢ - m\sum_{i=1}^{n}xᵢ - nb)$

Now we want to find the minimum by solving for b when the above expression is zero:

${∂D \over ∂m} = 0 = -2(\sum_{i=1}^{n}yᵢ - m\sum_{i=1}^{n}xᵢ - nb)$

$0 = \sum_{i=1}^{n}yᵢ - m\sum_{i=1}^{n}xᵢ - nb$

$0 = {1 \over n} \sum_{i=1}^{n}yᵢ - {m \over n}\sum_{i=1}^{n}xᵢ - b$

$b = {1 \over n} \sum_{i=1}^{n}yᵢ - {m \over n}\sum_{i=1}^{n}xᵢ$

We know that we can define the mean as:

$\overline x = {1 \over n} \sum_{i=1}^{n}xᵢ$

Using this definition, we can simplify our expression for b to:

$b = {1 \over n} \sum_{i=1}^{n}yᵢ - {m \over n}\sum_{i=1}^{n}xᵢ$

$b = {\overline y} - m{\overline x}$

This gives us a clean expression for `b` using just our sample means. Now let's try to do the same for m.

${∂D \over ∂m} = {∂ \over ∂b}\sum_{i=1}^{n}(yᵢ - mxᵢ - b)²$

${∂D \over ∂m} = \sum_{i=1}^{n}(-2xᵢ(yᵢ - mxᵢ - b)$

${∂D \over ∂m} = -2 \sum_{i=1}^{n}((xᵢyᵢ - mxᵢ² - bxᵢ)$

Now we want to solve for the value of `m` that minimizes D by solving for m when the partial is zero:

${∂D \over ∂m} = -2 \sum_{i=1}^{n}((xᵢyᵢ - mxᵢ² - bxᵢ)$

$0 = -2 \sum_{i=1}^{n}((xᵢyᵢ - mxᵢ² - bxᵢ)$

$0 = \sum_{i=1}^{n}((xᵢyᵢ - mxᵢ² - bxᵢ)$

We already know the value of b:

$b = {\overline y} - m{\overline x}$

We can replace this in our expression:

$0 = \sum_{i=1}^{n}((xᵢyᵢ - mxᵢ² - xᵢ({\overline y} - m{\overline x}))$

Then solve for `m`:

$0 = \sum_{i=1}^{n}((xᵢyᵢ - xᵢ({\overline y} - mxᵢ² - m{\overline x}))$

$0 = \sum_{i=1}^{n}((xᵢyᵢ - xᵢ({\overline y} - m(xᵢ² - {\overline x})))$

$\sum_{i=1}^{n}m(xᵢ² - {\overline x}) = \sum_{i=1}^{n}(xᵢyᵢ - xᵢ{\overline y})$

$m\sum_{i=1}^{n}(xᵢ² - {\overline x}) = \sum_{i=1}^{n}(xᵢyᵢ - xᵢ{\overline y})$

$m = {\sum_{i=1}^{n}(xᵢyᵢ - xᵢ{\overline y}) \over \sum_{i=1}^{n}(xᵢ² - {\overline x})}$

$m = {\sum_{i=1}^{n}xᵢyᵢ - n{\overline x}{\overline y} \over \sum_{i=1}^{n}xᵢ² - n{\overline x}²}$

This is a correct expression for `m`, but we can convert it to a more intuitive form using a couple of carefully selected identities. [[1]](http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf)

$\sum_{i=1}^{n}{\overline x}² - xᵢ{\overline x} = 0$

$\sum_{i=1}^{n}{\overline x}{\overline y} - yᵢ{\overline x} = 0$

Adding these to our previous expression, we can create:

$m = {\sum_{i=1}^{n}xᵢyᵢ - n{\overline x}{\overline y} \over \sum_{i=1}^{n}xᵢ² - n{\overline x}²}$

$m = {\sum_{i=1}^{n}xᵢyᵢ - n{\overline x}{\overline y} + \sum_{i=1}^{n}{\overline x}{\overline y} - yᵢ{\overline x} \over \sum_{i=1}^{n}xᵢ² - n{\overline x}² + \sum_{i=1}^{n}{\overline x}² - xᵢ{\overline x}}$

This can then factor to:

$m = {{1 \over n}\sum_{i=1}^{n}(xᵢ - {\overline x})(yᵢ - {\overline y}) \over {1 \over n}\sum_{i=1}^{n}(xᵢ² - {\overline x})²}$

$m = {\sum_{i=1}^{n}(xᵢ - {\overline x})(yᵢ - {\overline y}) \over \sum_{i=1}^{n}(xᵢ² - {\overline x})²}$

Noting the definitions for variance and covariance:

$Cov(x,y) = {\sum_{i=1}^{n}(xᵢ - {\overline x})(yᵢ - {\overline y}) \over n}$

$Var(x) = {\sum_{i=1}^{n}(xᵢ² - {\overline x}) \over n}$

We can also write `m` as:

$m = {Cov(x,y) \over Var(x)}$

Now we can put all of this derivation to use to create functions to perform linear regression for us.

```python
def E(x):
  """
  Returns the mean of a list of numbers
  
  >>> E([1,2,3])
  2.0
  >>> E([1,1,1])
  1.0
  """

  return sum(x)/len(x)

def cov(x,y):
  """
  Returns the covariance of a list

  >>> cov([2,3,4],[3,4,6])
  1.0
  """

  return E([(xᵢ - E(x))*(yᵢ - E(y)) for xᵢ,yᵢ in zip(x,y)])

def var(x):
  """
  Returns the variance of a list

  >>> var([2,3,4])
  0.6666666666666666
  """

  # This could alternatively simply return `cov(x,x)`
  return E([(xᵢ - E(x))**2 for xᵢ in x])
```

```python
def lin_reg(x,y):
  """ 
  Returns linear regression coefficients m and b

  >>> lin_reg([1,2,3],[2,4,6])
  (2.0, 0.0)
  """

  m = cov(x,y) / var(x)
  b = E(y) - m * E(x)

  return((m,b))

def pretty_lin_reg(x,y):
  m,b = lin_reg(x,y)

  return "y = %.06fx + %.06f" % (m,b)
```

Now we can calculate our linear regression equation:

```python
print(pretty_lin_reg(x,y))
```

Let's check our work using numpy:

```python
import numpy as np
import matplotlib.pyplot as plt

m,b = np.polyfit(x, y, 1)
print(m,b)
```

And graph our result just for fun:

```python
def plot_lin_reg(x,y):
  m,b = lin_reg(x,y)
  
  plt.scatter(x, y,  color='black')
  plt.plot(x, y, 'yo', x, np.poly1d((m,b))(x), '--k')
  plt.show()

plot_lin_reg(x,y)
```

Part B
------

> Find variance of x and variance of y. Please write your whole solution.

We discussed the definition of variance in part A as:

$Var(x) = {\sum_{i=1}^{n}(xᵢ² - {\overline x}) \over n}$

We can simply call the variance function that we created to compute regression coefficients to get our variances.

```python
var(x)
```

```python
var(y)
```

Problem 2
=========

> In the following table, x represents number of the years that an employee works in a company and y represent his/her salary.

```python
x = [5,3,4,10,15]
y = [25,20,21,35,38]
```

Part A
------

> Find the linear regression for x and y. Please write your whole solution.

We can simply reuse our derivations and functions from problem 1.

```python
pretty_lin_reg(x,y)
```

```python
plot_lin_reg(x,y)
```

Part B
------

> Find the variance of x and the variance of y. Please write your whole solution.

```python
var(x)
```

```python
var(y)
```

Problem 3
=========

> The following table shows number of absences in a class and the score of the students.

```python
x = [1,0,2,6,4,3,3]
y = [85,80,70,55,90,90,95]
```

Part A
------

> Find the linear regression between x and y. i.e. y = mx + b. Please write your whole
solution.

We can simply reuse our derivations and functions from problem 1.

```python
pretty_lin_reg(x,y)
```

```python
plot_lin_reg(x,y)
```

Part B
------

> Find the variance of x and the variance of y. Please write your whole solution.

```python
var(x)
```

```python
var(y)
```
